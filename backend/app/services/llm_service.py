from __future__ import annotations
from typing import Any, Dict, List, Callable, Awaitable, AsyncGenerator
import time
import logging
import httpx
import itertools
import os
from openai import AsyncOpenAI
from app.core.config import settings
from app.core.exceptions import LLMServiceError

logger = logging.getLogger(__name__)

# ======================================================
# å·¥å…·å‡½æ•°
# ======================================================
def safe_extract(js: dict, *path, default: str = "") -> str:
    try:
        val = js
        for p in path:
            val = val[p]
        return val or default
    except Exception:
        return default


# å¤ç”¨ httpx å®¢æˆ·ç«¯
_shared_client: httpx.AsyncClient | None = None

async def get_client() -> httpx.AsyncClient:
    global _shared_client
    if _shared_client is None:
        _shared_client = httpx.AsyncClient(timeout=settings.model_request_timeout)
    return _shared_client


# ======================================================
# Provider æ³¨å†Œ
# ======================================================
ProviderHandler = Callable[[List[Dict[str, str]], str | None], Awaitable[Dict[str, Any]]]
_registry: dict[str, ProviderHandler] = {}

def register_provider(name: str):
    def decorator(func: ProviderHandler):
        _registry[name.lower()] = func
        return func
    return decorator


# ======================================================
# Provider Key ç®¡ç†
# ======================================================
def _parse_provider_keys(raw: str | None) -> List[str]:
    """è§£æ provider_api_key å­—ç¬¦ä¸²ï¼Œæ”¯æŒé€—å·åˆ†éš”å¤š Key"""
    if not raw:
        return []
    return [k.strip() for k in raw.split(",") if k.strip()]


_PROVIDER_KEYS = _parse_provider_keys(settings.provider_api_key)
_PROVIDER_KEY_POOL = list(enumerate(_PROVIDER_KEYS, start=1))
_provider_key_cycle = itertools.cycle(_PROVIDER_KEY_POOL) if _PROVIDER_KEY_POOL else None


def _acquire_provider_key() -> tuple[int, str]:
    """
    è½®è¯¢è·å–ä¸‹ä¸€æŠŠ Provider Keyã€‚
    æ‰€æœ‰ä¾èµ–äº‘ç«¯/HTTP Provider çš„è°ƒç”¨éƒ½åº”ç»Ÿä¸€èµ°è¿™é‡Œï¼Œä»¥ä¾¿å¤š Key å‡åŒ€ä½¿ç”¨ã€‚
    """
    if _provider_key_cycle is None:
        raise LLMServiceError("æœªé…ç½® PROVIDER_API_KEYï¼Œè¯·è‡³å°‘æä¾›ä¸€æŠŠ Key")
    return next(_provider_key_cycle)


def _has_provider_key() -> bool:
    return bool(_PROVIDER_KEYS)


# ======================================================
# Gemini Provider
# ======================================================
@register_provider("gemini")
async def call_gemini(messages: List[Dict[str, str]], model_hint: str | None):
    model = model_hint or settings.llm_model_name or "gemini-2.5-flash"
    prompt = "\n".join([m.get("content", "") for m in messages])

    api_index, api_key = _acquire_provider_key()

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"thinkingConfig": {"thinkingBudget": 0}},
    }

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    headers = {"Content-Type": "application/json", "x-goog-api-key": api_key}

    client = await get_client()
    start = time.monotonic()

    try:
        resp = await client.post(url, json=payload, headers=headers)
        resp.raise_for_status()
    except httpx.HTTPStatusError as e:
        dur = (time.monotonic() - start) * 1000
        body = e.response.text[:300] if e.response is not None else ""
        logger.error(
            "[Gemini] HTTPé”™è¯¯ (API#%s) status=%s url=%s body=%s (%.1fms)",
            api_index,
            e.response.status_code if e.response else "unknown",
            url,
            body,
            dur,
        )
        return {
            "text": "",
            "raw": {
                "error": str(e),
                "status": e.response.status_code if e.response else None,
                "body": body,
            },
        }
    except Exception as e:
        dur = (time.monotonic() - start) * 1000
        logger.exception(f"[Gemini] è°ƒç”¨å¤±è´¥ (API#{api_index}) ({dur:.1f}ms)")
        return {"text": "", "raw": str(e)}

    js = resp.json()
    text = safe_extract(js, "candidates", 0, "content", "parts", 0, "text", default="")

    dur = (time.monotonic() - start) * 1000
    logger.info(f"[Gemini] æˆåŠŸ (API#{api_index}) {dur:.1f}ms text: {text[:50]}")

    return {"text": text, "raw": js}


# ======================================================
# Qwen Provider (DashScope å…¼å®¹æ¨¡å¼)
# ======================================================
@register_provider("qwen")
async def call_qwen(messages: List[Dict[str, str]], model_hint: str | None):
    model = model_hint or settings.llm_model_name or "qwen-plus"
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    url = f"{base_url}/chat/completions"

    api_index, api_key = _acquire_provider_key()

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    client = await get_client()
    start = time.monotonic()

    try:
        resp = await client.post(url, json=payload, headers=headers)
        resp.raise_for_status()
    except httpx.HTTPStatusError as e:
        dur = (time.monotonic() - start) * 1000
        body = e.response.text[:300] if e.response is not None else ""
        logger.error(
            "[Qwen] HTTPé”™è¯¯ (API#%s) status=%s url=%s body=%s (%.1fms)",
            api_index,
            e.response.status_code if e.response else "unknown",
            url,
            body,
            dur,
        )
        return {
            "text": "",
            "raw": {
                "error": str(e),
                "status": e.response.status_code if e.response else None,
                "body": body,
            },
        }
    except Exception as e:
        dur = (time.monotonic() - start) * 1000
        logger.exception(f"[Qwen] è°ƒç”¨å¤±è´¥ (API#{api_index}) ({dur:.1f}ms)")
        return {"text": "", "raw": str(e)}

    js = resp.json()
    text = safe_extract(js, "choices", 0, "message", "content", default="")

    dur = (time.monotonic() - start) * 1000
    logger.info(f"[Qwen] æˆåŠŸ (API#{api_index}) {dur:.1f}ms text: {text[:50]}")

    return {"text": text, "raw": js}


# ======================================================
# Qwen Provider æµå¼è°ƒç”¨ (DashScope å…¼å®¹æ¨¡å¼)
# ======================================================
async def call_qwen_stream(
    messages: List[Dict[str, str]], 
    model_hint: str | None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æµå¼è°ƒç”¨ Qwen Provider (DashScope å…¼å®¹æ¨¡å¼)
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        model_hint: æ¨¡å‹åç§°æç¤ºï¼ˆå¯é€‰ï¼‰
    
    Yields:
        æ¯ä¸ª chunk çš„å­—å…¸ï¼ŒåŒ…å«ï¼š
        - "text": å¢é‡æ–‡æœ¬å†…å®¹
        - "raw": åŸå§‹ chunk æ•°æ®
        - "done": æ˜¯å¦å®Œæˆï¼ˆæœ€åä¸€ä¸ª chunk ä¸º Trueï¼‰
    """
    model = model_hint or settings.llm_model_name or "qwen-plus"
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    
    api_index, api_key = _acquire_provider_key()
    
    # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ DashScope å…¼å®¹æ¨¡å¼ï¼‰
    client = AsyncOpenAI(
        api_key=api_key,
        base_url=base_url,
    )
    
    start = time.monotonic()
    accumulated_text = ""
    
    try:
        # å‘èµ·æµå¼è¯·æ±‚
        completion = await client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
            stream_options={"include_usage": True}
        )
        
        # é€æ­¥è¿”å›æ¯ä¸ª chunk
        async for chunk in completion:
            # æå–å¢é‡æ–‡æœ¬
            delta_text = ""
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    delta_text = delta.content
                    accumulated_text += delta_text
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå…¼å®¹å¤šç§æ–¹å¼ï¼‰
            try:
                if hasattr(chunk, 'model_dump'):
                    chunk_dict = chunk.model_dump()
                elif hasattr(chunk, 'dict'):
                    chunk_dict = chunk.dict()
                else:
                    # æ‰‹åŠ¨æ„å»ºå­—å…¸
                    chunk_dict = {
                        "id": getattr(chunk, 'id', None),
                        "object": getattr(chunk, 'object', None),
                        "created": getattr(chunk, 'created', None),
                        "model": getattr(chunk, 'model', None),
                        "choices": [
                            {
                                "index": getattr(choice, 'index', None),
                                "delta": {
                                    "content": getattr(choice.delta, 'content', None) if hasattr(choice, 'delta') else None
                                } if hasattr(choice, 'delta') else {},
                                "finish_reason": getattr(choice, 'finish_reason', None)
                            }
                            for choice in (chunk.choices or [])
                        ]
                    }
            except Exception as e:
                logger.warning(f"[Qwen Stream] è½¬æ¢ chunk ä¸ºå­—å…¸å¤±è´¥: {e}")
                chunk_dict = {"error": f"è½¬æ¢å¤±è´¥: {str(e)}"}
            
            # åˆ¤æ–­æ˜¯å¦å®Œæˆï¼ˆæ£€æŸ¥ finish_reasonï¼‰
            is_done = False
            if chunk.choices and len(chunk.choices) > 0:
                finish_reason = chunk.choices[0].finish_reason
                if finish_reason is not None:
                    is_done = True
            
            yield {
                "text": delta_text,
                "accumulated_text": accumulated_text,
                "raw": chunk_dict,
                "done": is_done
            }
        
        dur = (time.monotonic() - start) * 1000
        logger.info(f"[Qwen Stream] æˆåŠŸ (API#{api_index}) {dur:.1f}ms æ€»é•¿åº¦: {len(accumulated_text)}")
        
    except Exception as e:
        dur = (time.monotonic() - start) * 1000
        logger.exception(f"[Qwen Stream] è°ƒç”¨å¤±è´¥ (API#{api_index}) ({dur:.1f}ms)")
        # è¿”å›é”™è¯¯ä¿¡æ¯
        yield {
            "text": "",
            "accumulated_text": accumulated_text,
            "raw": {
                "error": str(e),
                "endpoint": base_url,
                "model": model,
            },
            "done": True
        }


# ======================================================
# æœ¬åœ° LLM è°ƒç”¨
# ======================================================
async def _call_local_llm(messages: List[Dict[str, str]], model_hint: str | None) -> Dict[str, Any]:
    if not settings.llm_service_url:
        raise LLMServiceError("æœ¬åœ° LLM URL æœªé…ç½®")

    client = await get_client()
    start = time.monotonic()

    # åˆ¤æ–­æœ¬åœ° vLLMï¼ˆ9020ç«¯å£ï¼‰
    is_vllm = any(tag in settings.llm_service_url for tag in [
        "localhost:9020", "127.0.0.1:9020", ":9020"
    ])

    # æå–å½“å‰ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(messages):
        if msg["role"] == "user":
            user_message = msg["content"]
            break
    if not user_message:
        raise LLMServiceError("æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")

    # æ„å»ºä¸Šä¸‹æ–‡
    context = []
    for msg in messages[:-1]:
        r, c = msg["role"], msg["content"]
        prefix = "ç”¨æˆ·" if r == "user" else "åŠ©æ‰‹"
        context.append(f"{prefix}: {c}")
    context_str = "\n".join(context)

    try:
        if is_vllm:
            # æœ¬åœ° vLLM
            resp = await client.post(
                f"{settings.llm_service_url}/chat",
                json={
                    "message": user_message,
                    "context": context_str,
                    "max_length": 512,
                    "temperature": 0.7
                }
            )
        else:
            # å…¶ä»–æœ¬åœ° LLM
            headers: Dict[str, str] = {}
            if _has_provider_key():
                _, bearer_key = _acquire_provider_key()
                headers["Authorization"] = f"Bearer {bearer_key}"

            resp = await client.post(
                f"{settings.llm_service_url}/chat-messages",
                json={"messages": messages, "model": model_hint},
                headers=headers
            )

        resp.raise_for_status()
    except Exception as e:
        logger.exception("[Local LLM] è°ƒç”¨å¤±è´¥")
        raise LLMServiceError(f"æœ¬åœ° LLM è°ƒç”¨å¤±è´¥: {e}")

    js = resp.json()
    text = js.get("response") or js.get("text") or safe_extract(js, "data", "text")

    return {"text": text, "raw": js}


# ======================================================
# æœ¬åœ° LLM æµå¼è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
# ======================================================
async def _call_local_llm_stream(
    messages: List[Dict[str, str]], 
    model_hint: str | None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æœ¬åœ° LLM æµå¼è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
    ç”±äº vLLM ä¸æ”¯æŒçœŸæ­£çš„æµå¼è¾“å‡ºï¼Œè¿™é‡Œé‡‡ç”¨å®šæ—¶åˆ†å—è¿”å›çš„æ–¹å¼æ¨¡æ‹Ÿæµå¼
    """
    if not settings.llm_service_url:
        raise LLMServiceError("æœ¬åœ° LLM URL æœªé…ç½®")
    
    # å…ˆè°ƒç”¨éæµå¼æ¥å£è·å–å®Œæ•´å“åº”
    try:
        full_result = await _call_local_llm(messages, model_hint)
        full_text = full_result.get("text", "")
    except Exception as e:
        logger.exception(f"[Local LLM Stream] è°ƒç”¨å¤±è´¥: {e}")
        yield {
            "text": "",
            "accumulated_text": "",
            "raw": {"error": str(e)},
            "done": True
        }
        return
    
    # æ¨¡æ‹Ÿæµå¼ï¼šå°†å®Œæ•´æ–‡æœ¬åˆ†å—è¿”å›
    # æŒ‰å­—ç¬¦æˆ–æŒ‰å¥å­åˆ†å‰²ï¼Œæ¯å—å»¶è¿Ÿä¸€å°æ®µæ—¶é—´
    import asyncio
    
    chunk_size = 3  # æ¯æ¬¡è¿”å› 3 ä¸ªå­—ç¬¦ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    accumulated = ""
    
    for i in range(0, len(full_text), chunk_size):
        chunk = full_text[i:i + chunk_size]
        accumulated += chunk
        
        yield {
            "text": chunk,
            "accumulated_text": accumulated,
            "raw": {"chunk_index": i // chunk_size},
            "done": False
        }
        
        # å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        await asyncio.sleep(0.05)  # 50ms å»¶è¿Ÿ
    
    # æœ€åè¿”å›å®Œæˆæ ‡è®°
    yield {
        "text": "",
        "accumulated_text": accumulated,
        "raw": {"done": True},
        "done": True
    }


# ======================================================
# Provider æµå¼æ³¨å†Œè¡¨
# ======================================================
ProviderStreamHandler = Callable[[List[Dict[str, str]], str | None], AsyncGenerator[Dict[str, Any], None]]
_stream_registry: dict[str, ProviderStreamHandler] = {}


def register_provider_stream(name: str):
    """æ³¨å†Œ Provider æµå¼å¤„ç†å™¨"""
    def decorator(func: ProviderStreamHandler):
        _stream_registry[name.lower()] = func
        return func
    return decorator


# æ³¨å†Œ Qwen æµå¼å¤„ç†å™¨
_stream_registry["qwen"] = call_qwen_stream


# ======================================================
# ğŸ”¥ğŸ”¥ æ–°å¢ä¸¤ä¸ªç¨³å®šå…¥å£ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
# ======================================================

async def chat_messages_local_stream(
    messages: List[Dict[str, str]], 
    *, 
    model_hint: str | None = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æ°¸è¿œèµ°æœ¬åœ° LLM æµå¼ï¼Œä¸çœ‹ provider é…ç½®
    è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€å—è¿”å›æ–‡æœ¬
    """
    async for chunk in _call_local_llm_stream(messages, model_hint):
        yield chunk


async def chat_messages_api_stream(
    messages: List[Dict[str, str]], 
    *, 
    model_hint: str | None = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æ°¸è¿œèµ°åœ¨çº¿ Provider API æµå¼
    è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€å—è¿”å›æ–‡æœ¬
    """
    provider = (settings.provider_name or "").lower()
    
    if provider not in _stream_registry:
        # å¦‚æœ Provider ä¸æ”¯æŒæµå¼ï¼Œå›é€€åˆ°éæµå¼å¹¶æ¨¡æ‹Ÿæµå¼è¿”å›
        logger.warning(f"[LLM Stream] Provider '{provider}' ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨éæµå¼æ¥å£å¹¶æ¨¡æ‹Ÿæµå¼è¿”å›")
        try:
            result = await chat_messages_api(messages, model_hint=model_hint)
            full_text = result.get("text", "")
            
            # æ¨¡æ‹Ÿæµå¼è¿”å›
            import asyncio
            chunk_size = 3
            accumulated = ""
            
            for i in range(0, len(full_text), chunk_size):
                chunk = full_text[i:i + chunk_size]
                accumulated += chunk
                yield {
                    "text": chunk,
                    "accumulated_text": accumulated,
                    "raw": {"chunk_index": i // chunk_size},
                    "done": False
                }
                await asyncio.sleep(0.05)
            
            yield {
                "text": "",
                "accumulated_text": accumulated,
                "raw": {"done": True},
                "done": True
            }
        except Exception as e:
            logger.exception(f"[LLM Stream] æ¨¡æ‹Ÿæµå¼å¤±è´¥: {e}")
            yield {
                "text": "",
                "accumulated_text": "",
                "raw": {"error": str(e)},
                "done": True
            }
        return
    
    # ä½¿ç”¨æ³¨å†Œçš„æµå¼å¤„ç†å™¨
    handler = _stream_registry[provider]
    async for chunk in handler(messages, model_hint):
        yield chunk


async def chat_messages_stream(
    messages: List[Dict[str, str]], 
    *, 
    model_hint: str | None = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æ™ºèƒ½é€‰æ‹© LLM é€šé“ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰ï¼š
    - è‹¥æ£€æµ‹åˆ°æœ¬åœ° llm_service_urlï¼Œä¼˜å…ˆèµ°æœ¬åœ°ï¼ˆä½æ—¶å»¶ã€å¯ç¦»çº¿ï¼‰
    - å¦åˆ™å›é€€åˆ°äº‘ç«¯ Providerï¼ˆä¾æ® provider_nameï¼‰
    
    è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€å—è¿”å›æ–‡æœ¬
    """
    if settings.llm_service_url:
        async for chunk in chat_messages_local_stream(messages, model_hint=model_hint):
            yield chunk
    else:
        async for chunk in chat_messages_api_stream(messages, model_hint=model_hint):
            yield chunk


# ======================================================
# ğŸ”¥ğŸ”¥ æ–°å¢ä¸¤ä¸ªç¨³å®šå…¥å£ï¼ˆéæµå¼ç‰ˆæœ¬ï¼Œä¿æŒä¸å˜ï¼‰
# ======================================================

async def chat_messages_local(messages: List[Dict[str, str]], *, model_hint: str | None = None):
    """æ°¸è¿œèµ°æœ¬åœ° LLMï¼Œä¸çœ‹ provider é…ç½®"""
    return await _call_local_llm(messages, model_hint)


async def chat_messages_api(messages: List[Dict[str, str]], *, model_hint: str | None = None):
    """æ°¸è¿œèµ°åœ¨çº¿ Provider API"""
    provider = (settings.provider_name or "").lower()
    if provider not in _registry:
        raise LLMServiceError("æœªé…ç½® provider_name æˆ–è¯¥ provider æœªæ³¨å†Œ")
    handler = _registry[provider]
    return await handler(messages, model_hint)


async def chat_messages(messages: List[Dict[str, str]], *, model_hint: str | None = None):
    """
    æ™ºèƒ½é€‰æ‹© LLM é€šé“ï¼š
    - è‹¥æ£€æµ‹åˆ°æœ¬åœ° llm_service_urlï¼Œä¼˜å…ˆèµ°æœ¬åœ°ï¼ˆä½æ—¶å»¶ã€å¯ç¦»çº¿ï¼‰
    - å¦åˆ™å›é€€åˆ°äº‘ç«¯ Providerï¼ˆä¾æ® provider_nameï¼‰
    """
    if settings.llm_service_url:
        return await chat_messages_local(messages, model_hint=model_hint)
    return await chat_messages_api(messages, model_hint=model_hint)
